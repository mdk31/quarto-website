[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Website",
    "section": "",
    "text": "machine learning\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal inference\n\n\npresentation\n\n\n\n\nMay 2022 ACIC poster presentation\n\n\n\n\n\n\nAug 5, 2022\n\n\nMatt Kosko\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m currently a data scientist in Washington, DC and I’m currently working on a project applying bootstrap techniques to causal inference."
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "My Website",
    "section": "",
    "text": "machine learning\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal inference\n\n\npresentation\n\n\n\n\nMay 2022 ACIC poster presentation\n\n\n\n\n\n\nAug 5, 2022\n\n\nMatt Kosko\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Scalable Bootstrap Algorithms for Causal Inference with Large Real-World Data",
    "section": "",
    "text": "We are well-past the 2022 ACIC conference but, since I’m starting a new website and don’t have anything to post, I’d figure I’d show what I’m currently working on. This is the poster I presented at the conference in May. It’s still a work in progress and I’m currently working with Michele Santacatterina and Lin Wang on finalizing the paper."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I’m currently a data scientist in Washington, DC and I’m currently working on a project applying bootstrap techniques to causal inference.\nYou can also find out more about me on LinkedIn."
  },
  {
    "objectID": "posts/ho/index.html",
    "href": "posts/ho/index.html",
    "title": "Hyperparameter Optimization",
    "section": "",
    "text": "Hyperparameter optimization is a critical aspect of machine learning. When optimizing hyperparameters, our objective is to discover the ideal configuration of hyperparameters for a given model. While standard parameters are adjusted during the training process based on data, hyperparameters, such as learning rates or regularization terms, are set prior to the learning phase.\nThe standard technique for hyperparameter optimization (HO) is grid search where every possible combination of hyperparameters is tested. Because of the vast potential combinations of hyperparameters, random search, where a random subset of hyperparameters is used, is often attempted. The problem with either of these techniques is that they require you to train multiple models that will later be discarded (Mlodozeniec, Reisser, and Louizos 2023, 1). Moreover, neither of these methods are scalable to cases with many hyperparameters (Lorraine, Vicol, and Duvenaud 2020; Maclaurin, Duvenaud, and Adams 2015)\nIt would be preferable if we could update hyperparameters in the same way we update model parameters, i.e., through the use of gradient-based methods. Ideally, we would calculate the derivative of the loss function with respect to each hyperparameter and then update the hyperparameters, i.e.:\n\\[\n\\lambda_{t+1} = \\lambda_t - \\alpha\\dfrac{\\partial\\mathcal{L}}{\\partial \\lambda}\n\\]\nLuckily, there have been a variety of papers published proposing gradient-based methods to update hyperparameters. In this post, I will apply the gradient-based HO outline from Lorraine, Vicol, and Duvenaud (2020) to a simple neural network with a single weight decay hyperparameter. I thought this would be a good test case to get more familiar with PyTorch, which makes much of the computation extremely easy and efficient."
  },
  {
    "objectID": "posts/ho/index.html#computation",
    "href": "posts/ho/index.html#computation",
    "title": "Hyperparameter Optimization",
    "section": "Computation",
    "text": "Computation\nThe issue now is purely computational: how can we quickly calculate the Hessian matrix and its inverse in the above equation. (Pedregosa 2016) suggests using the conjugate gradient method to find:\n\\[\n\\left(\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^{-1}\\dfrac{\\partial \\mathcal{L}_V(\\boldsymbol\\lambda, \\mathbf{w}^\\ast(\\boldsymbol{\\lambda}))}{\\partial \\mathbf{w}^\\ast}\n\\]\nIn other words, solve:\n\\[\n\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T} \\mathbf{x} = \\dfrac{\\partial \\mathcal{L}_V(\\boldsymbol\\lambda, \\mathbf{w}^\\ast(\\boldsymbol{\\lambda}))}{\\partial \\mathbf{w}^\\ast}\n\\]\nThis can be done without ever actually instantiating the Hessian matrix; it only requires a Hessian vector product, which is easy to calculate in PyTorch.\n(Lorraine, Vicol, and Duvenaud 2020) instead proposes the calculation of the first several terms of the Neumann series:\n\\[\n\\left(\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^{-1} = \\lim_{i \\to \\infty} \\displaystyle\\sum_{j=0}^i\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^j\n\\]\nThey propose a clever algorithm that calculates the terms of this series recursively and does not require instantiating the second derivative matrix, only the vector-Hessian products.\n\\[\n\\begin{align*}\nv_0 &= \\dfrac{\\partial \\mathcal{L}_V}{\\partial \\mathbf{w}} \\\\\nv_1 &= v_0 - v_0 \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T} = v_0\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right) \\\\\nv_2 &= v_1 - v_1 \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T} = v_1\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right) = v_0\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right)^2 \\\\\n&\\ldots\n\\end{align*}\n\\]\nThis is simple to implement in PyTorch because of the ease in calculating vector-Jacobian products. Here is an example where we calculate the Hessian inverse of:\n\\[\nx_1^3 + x_2^3, \\quad x_1 = x_2 = 0.1\n\\]\n\nimport torch\nfrom torch.autograd.functional import hessian\n\ndef f(x):\n    return x.pow(3).sum()\n\ndef neumann_series(v, f, I=50):\n    v = v.detach().clone()\n    p = v.detach().clone()\n    for i in range(I):\n        tmp_v = torch.autograd.grad(f, inputs, grad_outputs=v, retain_graph=True)\n        tmp_v = torch.cat([grad.view(-1) for grad in tmp_v])\n        v = v - tmp_v\n        p = p + v\n    return p\n\nv = torch.tensor([1, 1], dtype=torch.float32)\ninputs = torch.tensor([0.1, 0.1], requires_grad=True)\nhess = hessian(f, inputs)\ninvhess = torch.linalg.inv(hess)\n\ny = f(inputs)\ngrad_y = torch.autograd.grad(y, inputs, create_graph=True)\nneumann_approx = neumann_series(v = v, f = grad_y, I = 50)\ntorch.isclose(v @ invhess, neumann_approx)\n\ntensor([True, True])\n\n\nA sufficient condition for the Neumann series to converge is that the spectral radius (maximum of the eigenvalues) is less than 1. While the above code works, in practice most of the contrived neural network examples I created in Pytorch did not satisfy this sufficient condition and the Neumann series rarely converged. As a result, I decided to stick with the conjugate gradient method."
  },
  {
    "objectID": "posts/ho/index.html#drawbacks",
    "href": "posts/ho/index.html#drawbacks",
    "title": "Hyperparameter Optimization",
    "section": "Drawbacks",
    "text": "Drawbacks\nThe main drawback to this methodology is that it requires the Hessian to be invertible, even if you don’t have to calculate it exactly. In practice, the Hessian is rarely invertible in deep learning problems (Sagun, Bottou, and LeCun 2016). In addition, the conjugate gradient descent method I use is also rather slow; the HO gradient method took about 10 minutes to run. Despite that, I think this method has real advantages as it can converge to the correct \\(\\lambda\\). Grid search, by contrast, will never find this \\(\\lambda\\) unless it is part of the set of considered hyperparameters."
  }
]