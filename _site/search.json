[
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "My Website",
    "section": "Posts",
    "text": "Posts"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Scalable Bootstrap Algorithms for Causal Inference with Large Real-World Data",
    "section": "",
    "text": "We are well-past the 2022 ACIC conference but, since I’m starting a new website and don’t have anything to post, I’d figure I’d show what I’m currently working on. This is the poster I presented at the conference in May. It’s still a work in progress and I’m currently working with Michele Santacatterina and Lin Wang on finalizing the paper."
  },
  {
    "objectID": "posts/ho/index.html",
    "href": "posts/ho/index.html",
    "title": "Hyperparameter Optimization",
    "section": "",
    "text": "Hyperparameter optimization is a critical aspect of machine learning. When optimizing hyperparameters, our objective is to discover the ideal configuration of hyperparameters for a given model. While standard parameters are adjusted during the training process based on data, hyperparameters, such as learning rates or regularization terms, are set prior to the learning phase.\nThe standard technique for hyperparameter optimization (HO) is grid search where every possible combination of hyperparameters is tested. Because of the vast potential combinations of hyperparameters, random search, where a random subset of hyperparameters is used, is often attempted. The problem with either of these techniques is that they require you to train multiple models that will later be discarded (Mlodozeniec, Reisser, and Louizos 2023, 1). Moreover, neither of these methods are scalable to cases with many hyperparameters (Lorraine, Vicol, and Duvenaud 2020; Maclaurin, Duvenaud, and Adams 2015)\nIt would be preferable if we could update hyperparameters in the same way we update model parameters, i.e., through the use of gradient-based methods. Ideally, we would calculate the derivative of the loss function with respect to each hyperparameter and then update the hyperparameters, i.e.:\n\\[\n\\lambda_{t+1} = \\lambda_t - \\alpha\\dfrac{\\partial\\mathcal{L}}{\\partial \\lambda}\n\\]\nLuckily, there have been a variety of papers published proposing gradient-based methods to update hyperparameters. In this post, I will apply the gradient-based HO outline from Lorraine, Vicol, and Duvenaud (2020) to a simple neural network with a single weight decay hyperparameter. I thought this would be a good test case to get more familiar with PyTorch, which makes much of the computation extremely easy and efficient."
  },
  {
    "objectID": "posts/ho/index.html#computation",
    "href": "posts/ho/index.html#computation",
    "title": "Hyperparameter Optimization",
    "section": "Computation",
    "text": "Computation\nThe issue now is purely computational: how can we quickly calculate the Hessian matrix and its inverse in the above equation. (Pedregosa 2016) suggests using the conjugate gradient method to find:\n\\[\n\\left(\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^{-1}\\dfrac{\\partial \\mathcal{L}_V(\\boldsymbol\\lambda, \\mathbf{w}^\\ast(\\boldsymbol{\\lambda}))}{\\partial \\mathbf{w}^\\ast}\n\\]\nIn other words, solve:\n\\[\n\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T} \\mathbf{x} = \\dfrac{\\partial \\mathcal{L}_V(\\boldsymbol\\lambda, \\mathbf{w}^\\ast(\\boldsymbol{\\lambda}))}{\\partial \\mathbf{w}^\\ast}\n\\]\nThis can be done without ever actually instantiating the Hessian matrix; it only requires a Hessian vector product, which is easy to calculate in PyTorch.\n(Lorraine, Vicol, and Duvenaud 2020) instead proposes the calculation of the first several terms of the Neumann series:\n\\[\n\\left(\\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^{-1} = \\lim_{i \\to \\infty} \\displaystyle\\sum_{j=0}^i\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w}\\partial \\mathbf{w}^T}\\right)^j\n\\]\nThey propose a clever algorithm that calculates the terms of this series recursively and does not require instantiating the second derivative matrix, only the vector-Hessian products.\n\\[\n\\begin{align*}\nv_0 &= \\dfrac{\\partial \\mathcal{L}_V}{\\partial \\mathbf{w}} \\\\\nv_1 &= v_0 - v_0 \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T} = v_0\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right) \\\\\nv_2 &= v_1 - v_1 \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T} = v_1\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right) = v_0\\left(I - \\dfrac{\\partial^2 \\mathcal{L}_T}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}\\right)^2 \\\\\n&\\ldots\n\\end{align*}\n\\]\nThis is simple to implement in PyTorch because of the ease in calculating vector-Jacobian products. To illustrate, the following is a simple example where we use the Neumann approximation to calculate the Hessian inverse of:\n\\[\nx_1^3 + x_2^3, \\quad x_1 = x_2 = 0.1\n\\]\n\nimport torch\nfrom torch.autograd.functional import hessian\n\ndef f(x):\n    return x.pow(3).sum()\n\ndef neumann_series(v, f, I=50):\n    v = v.detach().clone()\n    p = v.detach().clone()\n    for i in range(I):\n        tmp_v = torch.autograd.grad(f, inputs, grad_outputs=v, retain_graph=True)\n        tmp_v = torch.cat([grad.view(-1) for grad in tmp_v])\n        v = v - tmp_v\n        p = p + v\n    return p\n\nv = torch.tensor([1, 1], dtype=torch.float32)\ninputs = torch.tensor([0.1, 0.1], requires_grad=True)\nhess = hessian(f, inputs)\ninvhess = torch.linalg.inv(hess)\n\ny = f(inputs)\ngrad_y = torch.autograd.grad(y, inputs, create_graph=True)\nneumann_approx = neumann_series(v = v, f = grad_y, I = 50)\ntorch.isclose(v @ invhess, neumann_approx)\n\ntensor([True, True])\n\n\nA sufficient condition for the Neumann series to converge is that the spectral radius (maximum of the eigenvalues) is less than 1. While the above code works, in practice most of the contrived neural network examples I created in Pytorch did not satisfy this sufficient condition and the Neumann series rarely converged. As a result, I decided to stick with the conjugate gradient method."
  },
  {
    "objectID": "posts/ho/index.html#drawbacks",
    "href": "posts/ho/index.html#drawbacks",
    "title": "Hyperparameter Optimization",
    "section": "Drawbacks",
    "text": "Drawbacks\nThe main drawback to this methodology is that it requires the Hessian to be invertible, even if you don’t have to calculate it exactly. In practice, the Hessian is rarely invertible in deep learning problems (Sagun, Bottou, and LeCun 2016). In addition, the conjugate gradient descent method I use is also rather slow; the HO gradient method took about 10 minutes to run. Despite that, I think this method has real advantages as it can converge to the correct \\(\\lambda\\). Grid search, by contrast, will never find this \\(\\lambda\\) unless it is part of the set of considered hyperparameters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m currently a data scientist in Washington DC and working on a project applying bootstrap techniques to causal inference."
  },
  {
    "objectID": "posts/crossval/index.html",
    "href": "posts/crossval/index.html",
    "title": "Generalization Error Estimation Methods for Fine-Tuned Models",
    "section": "",
    "text": "One of the great advantages of the large increase in pre-trained transformer-based large language models (LLMs) like BERT and GPT is the ability of these models to be fruitfully applied to a variety of tasks they were not initially trained on (Wei, Xie, and Ma 2021). In particular, these models can be fine-tuned for other tasks, using datasets that are much smaller than ones used train the original model; this is attractive precisely because you can leverage the enormous amount of information the model learned during pre-training on a task where labeled data is expensive to obtain (Church, Chen, and Ma 2021).\nAs with any task, once we fit a model we want to measure how well it performs, that is, we want to validate it. Those using LLMs in downstream tasks should remember that typical model validation techniques often used in large sample-size cases do not work as well when datasets are small. In particular, when datasets are too small for train/test spliting, you can make use of resampling methods like K-fold cross validation. In this post, we explore how resampling methods can improve validation estimation compared to data-splitting in the small N case."
  },
  {
    "objectID": "posts/crossval/index.html#data-splitting",
    "href": "posts/crossval/index.html#data-splitting",
    "title": "Generalization Error Estimation Methods for Fine-Tuned Models",
    "section": "Data-Splitting",
    "text": "Data-Splitting\nThe most straightforward method for validating a model is data-splitting, the classic train/test split method introduced in every machine learning textbook. The data is divided into a train set \\(\\mathcal{T}\\) and a test set \\(\\mathcal{R}\\) (Harrell et al. 2001; Ghojogh and Crowley 2019). When the test set is held out, the resulting error estimates Equation 1 rather than Equation 2."
  },
  {
    "objectID": "posts/crossval/index.html#k-fold-cross-validation",
    "href": "posts/crossval/index.html#k-fold-cross-validation",
    "title": "Generalization Error Estimation Methods for Fine-Tuned Models",
    "section": "K-Fold Cross-validation",
    "text": "K-Fold Cross-validation\nIn K-fold cross validation, the data is split into \\(K\\) folds and then, for each fold, the model is trained on \\(K-1\\) folds. A performance metric is estimated on each holdout fold and these metrics are then averaged over all folds. Repeat K-fold is the same procedure except it repeats the K-fold procedure multiple times. Because the training data is randomly partitioned into folds, it is not held constant and these methods estimate Equation 2."
  }
]