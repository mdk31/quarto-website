<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Matt Kosko’s Website - Hyperparameter Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Matt Kosko’s Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/" rel="" target="">
 <span class="menu-text">Posts</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mdk31" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Hyperparameter Optimization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Hyperparameter optimization is a critical aspect of machine learning, where the objective is to discover the ideal configuration settings, or “hyperparameters”, for a given model. While standard parameters are adjusted during the training process based on data, hyperparameters, such as learning rates or regularization terms, are set prior to this phase. These can include the weight decay parameter, the number of hidden layers, etc. The goal is to find hyperparameters that result in the best model performance, often measured by validation accuracy or loss.</p>
<p>The standard technique for hyperparamter optimization (HO) is grid search, where every possible combination of hyperparameters is tested. Because of the vast potential combinations of hyperparameters, you random search is often employed where RANDOM. The problem with either of these techniques is that they require you to train multiple models that will later be discarded <span class="citation" data-cites="mlodozeniec2023hyperparameter">(<a href="#ref-mlodozeniec2023hyperparameter" role="doc-biblioref">Mlodozeniec, Reisser, and Louizos 2023, 1</a>)</span>. Moreover, these methods are not scalable to cases with many hyperparameters <span class="citation" data-cites="lorraine2020optimizing">(<a href="#ref-lorraine2020optimizing" role="doc-biblioref">Lorraine, Vicol, and Duvenaud 2020</a>)</span> <span class="citation" data-cites="maclaurin2015gradient">(<a href="#ref-maclaurin2015gradient" role="doc-biblioref">Maclaurin, Duvenaud, and Adams 2015</a>)</span></p>
<p>It would be preferable if we could update hyperparameters in the same way we update model parameters, i.e., through the use of gradient-based methods. Similar to updates to parameters, and differently from grid search methods, we hope that we can make use of previous evaluation to “make an informed decision of the next iterate” <span class="citation" data-cites="pedregosa2016hyperparameter">(<a href="#ref-pedregosa2016hyperparameter" role="doc-biblioref">Pedregosa 2016</a>)</span>. In this post, I will illustrate the application of gradient-based HO methods to a simple neural network to optimize the weight decay hyperparameter. I thought this would be a good test case to get more familiar with PyTorch.</p>
</section>
<section id="gradient-computation" class="level1">
<h1>Gradient Computation</h1>
<p>The first hurdle to overcome in gradient HO is the fact that you cannot optimize hyperparameter to minimize the same loss function as the parameters; in the example of weight decay, this would always lead to preferring models with no penalization <span class="citation" data-cites="pedregosa2016hyperparameter">(<a href="#ref-pedregosa2016hyperparameter" role="doc-biblioref">Pedregosa 2016</a>)</span>. As a result, we need to use cross-validation or a holdout validation set to optimize a criterion on unseen data. We use a validation set and construct HO as a two-level optimization problem as in <span class="citation" data-cites="pedregosa2016hyperparameter">Pedregosa (<a href="#ref-pedregosa2016hyperparameter" role="doc-biblioref">2016</a>)</span> and <span class="citation" data-cites="lorraine2020optimizing">Lorraine, Vicol, and Duvenaud (<a href="#ref-lorraine2020optimizing" role="doc-biblioref">2020</a>)</span>. Let <span class="math inline">\(\mathcal{L}_T\)</span> and <span class="math inline">\(\mathcal{L}_V\)</span> be the training and validation loss, respectively. Both of these depend on the hyperparamters <span class="math inline">\(\boldsymbol\lambda\)</span> and model parameters <span class="math inline">\(\mathbf{w}\)</span>, whether directly or indircetly (more on this later). First, we train the model and update the model parameters for a given value of <span class="math inline">\(\boldsymbol\lambda\)</span>. Then we update the hyperparameters:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol\lambda^\ast &amp;\equiv \mathop{\mathrm{arg\,min}}\mathcal{L}_V^\ast(\boldsymbol\lambda), \quad \textrm{where} \\
\mathcal{L}_V^\ast(\boldsymbol\lambda) &amp;\equiv \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol\lambda)) \, \textrm{and} \, \mathbf{w}^\ast(\boldsymbol\lambda) \equiv \mathop{\mathrm{arg\,min}}\mathcal{L}_T(\boldsymbol\lambda, \mathbf{w})
\end{align}
\]</span></p>
<p>The second issue is more difficult to handle. The validation loss can depend on the hyperparameters both directly but also indirectly through the optimal weights <span class="math inline">\(w^\ast\)</span>. We have to calculate how the optimal weights change as the hyperparameters change <span class="citation" data-cites="lorraine2020optimizing">(<a href="#ref-lorraine2020optimizing" role="doc-biblioref">Lorraine, Vicol, and Duvenaud 2020</a>)</span>.</p>
<p>Following <span class="citation" data-cites="lorraine2020optimizing">(<a href="#ref-lorraine2020optimizing" role="doc-biblioref">Lorraine, Vicol, and Duvenaud 2020</a>)</span>, the training loss is an implicit function of <span class="math inline">\(\boldsymbol\lambda\)</span>. Under certain regularity conditions, the optimal values of the parameters are given by the equation (implicity a function of <span class="math inline">\(\boldsymbol{\lambda}\)</span>):</p>
<p><span class="math display">\[
\dfrac{\partial \mathcal{L}_T}{\partial \mathbf{w}} \Bigg|_{\boldsymbol{\lambda},\mathbf{w}^\ast(\boldsymbol{\lambda})} = 0
\]</span></p>
<p>The implicit equation with respect to <span class="math inline">\(\boldsymbol{\lambda}\)</span> is then:</p>
<p><span class="math display">\[
\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \boldsymbol{\lambda}^T} + \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\dfrac{\partial \mathbf{w}^\ast}{\partial \boldsymbol{\lambda}} = 0
\]</span></p>
<p>Assuming that the derivative of <span class="math inline">\(\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\)</span> exists, we can decompose the validation gradient with respect to <span class="math inline">\(\boldsymbol{\lambda}\)</span> as:</p>
<p><span class="math display">\[
\begin{align*}
\dfrac{\partial \mathcal{L}_V^\ast(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \boldsymbol{\lambda}} &amp;= \dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \boldsymbol{\lambda}} + \dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \mathbf{w}^\ast}  \dfrac{\partial \mathbf{w}^\ast}{\partial \boldsymbol{\lambda}}\\
&amp;= \dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \boldsymbol{\lambda}} - \left(\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \boldsymbol{\lambda}^T}\right)^T \left(\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\right)^{-1}\dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \mathbf{w}^\ast}
\end{align*}
\]</span></p>
<p>So, by placing regularity conditions on the training loss, we can calculate the gradient of the validation loss with respect to the hyperparameters and use that to iteratively update these hyperparameters.</p>
<section id="computation" class="level2">
<h2 class="anchored" data-anchor-id="computation">Computation</h2>
<p>The issue noew is purely computational: how can we quickly calculate the Hessian matrix and its inverse in the above equation. The two papers both lay out how to calculate an approximate solution to the inverse Hessian-vector product:</p>
<p><span class="citation" data-cites="pedregosa2016hyperparameter">(<a href="#ref-pedregosa2016hyperparameter" role="doc-biblioref">Pedregosa 2016</a>)</span> suggest using the conjugate gradient method to find</p>
<p><span class="math display">\[
\left(\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\right)^{-1}\dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \mathbf{w}^\ast}
\]</span></p>
<p>In other words, solve:</p>
<p><span class="math display">\[
\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T} \mathbf{x} = \dfrac{\partial \mathcal{L}_V(\boldsymbol\lambda, \mathbf{w}^\ast(\boldsymbol{\lambda}))}{\partial \mathbf{w}^\ast}
\]</span> HOAG only requires Hessian vector product so never need ot instantiate Hessian (can cite both HOAG and Lorraine paper)</p>
<p><span class="citation" data-cites="lorraine2020optimizing">(<a href="#ref-lorraine2020optimizing" role="doc-biblioref">Lorraine, Vicol, and Duvenaud 2020</a>)</span> instead proposes the calcualtion of the first several terms of the Neumann series:</p>
<p><span class="math display">\[
\left(\dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\right)^{-1} = \lim_{i \to \infty} \displaystyle\sum_{j=0}^i\left(I - \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w}\partial \mathbf{w}^T}\right)^j
\]</span></p>
<p>The propose a clever algorithm that calculates the terms of this series recursively and do not require instantiating the second derivative matrix, only the vector-Hessian products.</p>
<p><span class="math display">\[
\begin{align*}
v_0 &amp;= \dfrac{\partial \mathcal{L}_V}{\partial \mathbf{w}} \\
v_1 &amp;= v_0 - v_0 \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \mathbf{w}^T} = v_0\left(I - \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \mathbf{w}^T}\right) \\
v_2 &amp;= v_1 - v_1 \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \mathbf{w}^T} = v_1\left(I - \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \mathbf{w}^T}\right) = v_0\left(I - \dfrac{\partial^2 \mathcal{L}_T}{\partial \mathbf{w} \partial \mathbf{w}^T}\right)^2 \\
&amp;\ldots
\end{align*}
\]</span> This is simple to implement in PyTorch because of the ease in calculating vector-Jacobian products.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import torch</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from torch.autograd.functional import hessian</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># def f(x):</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     return x.pow(3).sum()</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># def neumann_series(v, f, I=50):</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     v = v.detach().clone()</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#     p = v.detach().clone()</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#     for i in range(I):</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#         tmp_v = torch.autograd.grad(f, inputs, grad_outputs=v, retain_graph=True)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#         tmp_v = torch.cat([grad.view(-1) for grad in tmp_v])</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#         v = v - tmp_v</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#         p = p + v</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     return p</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># v = torch.tensor([1, 1], dtype=torch.float32)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs = torch.tensor([0.1, 0.1], requires_grad=True)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># hess = hessian(f, inputs)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># invhess = torch.linalg.inv(hess)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># v @ invhess</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># y = f(inputs)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># grad_y = torch.autograd.grad(y, inputs, create_graph=True)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># v = v.detach().clone()</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># p = v.detach().clone()</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in range(50):</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">#     tmp_v = torch.autograd.grad(grad_y, inputs, grad_outputs=v, retain_graph=True)</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">#     tmp_v = torch.cat([grad.view(-1) for grad in tmp_v])</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">#     v = v - tmp_v</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">#     p = p + v</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In practice, most of the contrived examples I created did not satisfy this sufficient condition and the Neumann series rarely converged.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>The full code can be found in the Github <a href="https://github.com/mdk31/hyperparameter-optimization">repository</a></p>
<p>Figure <a href="#fig-grid">Figure&nbsp;1</a> shows</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> <span class="op">-</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-grid" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<pre><code>1</code></pre>
<figcaption class="figure-caption">Figure&nbsp;1: Validation error trajectory by lambda</figcaption>
</figure>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> <span class="op">-</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-hoag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<pre><code>1</code></pre>
<figcaption class="figure-caption">Figure&nbsp;2: Validation error trajectory by starting lambda</figcaption>
</figure>
</div>
</div>
<p>The rapid convergence of the <span class="math inline">\(\lambda\)</span> to the low <span class="math inline">\(\lambda\)</span> values (and the consequent drop in the validation error) is striking. The actual <span class="citation" data-cites="pedregosa2016hyperparameter">(<a href="#ref-pedregosa2016hyperparameter" role="doc-biblioref">Pedregosa 2016</a>)</span> has a complex set of tolerances that vary over update epochs to ensure theroetical convergence of the HOAG algorithm as a whole. Even without that, it performs well with fixed tolerances.</p>
<section id="drawbacks" class="level2">
<h2 class="anchored" data-anchor-id="drawbacks">Drawbacks</h2>
<p>The main drawback to this methodology is that it requires the Hessian to be invertible, even if you don’t have to calculate it exactly. In practice, the Hessian is rarely invertible in deep learning problems <span class="citation" data-cites="sagun2016eigenvalues">(<a href="#ref-sagun2016eigenvalues" role="doc-biblioref">Sagun, Bottou, and LeCun 2016</a>)</span>.</p>
<p>The conjugate gradient descent method I use is also rather slow; the HOAG method took about 10 minutes to run.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-lorraine2020optimizing" class="csl-entry" role="listitem">
Lorraine, Jonathan, Paul Vicol, and David Duvenaud. 2020. <span>“Optimizing Millions of Hyperparameters by Implicit Differentiation.”</span> In <em>International Conference on Artificial Intelligence and Statistics</em>, 1540–52. PMLR.
</div>
<div id="ref-maclaurin2015gradient" class="csl-entry" role="listitem">
Maclaurin, Dougal, David Duvenaud, and Ryan Adams. 2015. <span>“Gradient-Based Hyperparameter Optimization Through Reversible Learning.”</span> In <em>International Conference on Machine Learning</em>, 2113–22. PMLR.
</div>
<div id="ref-mlodozeniec2023hyperparameter" class="csl-entry" role="listitem">
Mlodozeniec, Bruno, Matthias Reisser, and Christos Louizos. 2023. <span>“Hyperparameter Optimization Through Neural Network Partitioning.”</span> <em>arXiv Preprint arXiv:2304.14766</em>.
</div>
<div id="ref-pedregosa2016hyperparameter" class="csl-entry" role="listitem">
Pedregosa, Fabian. 2016. <span>“Hyperparameter Optimization with Approximate Gradient.”</span> In <em>International Conference on Machine Learning</em>, 737–46. PMLR.
</div>
<div id="ref-sagun2016eigenvalues" class="csl-entry" role="listitem">
Sagun, Levent, Leon Bottou, and Yann LeCun. 2016. <span>“Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond.”</span> <em>arXiv Preprint arXiv:1611.07476</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>